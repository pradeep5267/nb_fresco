{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run the Cell to import the packages and data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import fashion_mnist\n",
    "from keras.utils import to_categorical\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Loading**\n",
    "\n",
    "Run the below cells to load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
      "32768/29515 [=================================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
      "26427392/26421880 [==============================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
      "8192/5148 [===============================================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
      "4423680/4422102 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "(trainX, trainy), (testX, testy) = fashion_mnist.load_data()\n",
    "# load train and test dataset\n",
    "def load_dataset():\n",
    "    # load dataset\n",
    "    (trainX, trainy), (testX, testY) = fashion_mnist.load_data()\n",
    "    # reshape dataset to have a single channel\n",
    "    trainX = trainX.reshape((trainX.shape[0], 28, 28, 1))\n",
    "    testX = testX.reshape((testX.shape[0], 28, 28, 1))\n",
    "    # one hot encode target values\n",
    "    trainy = to_categorical(trainy)\n",
    "    testY = to_categorical(testY)\n",
    "    return trainX, trainy, testX, testY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Subset Generation**\n",
    "\n",
    "- Perform data split  with **StratifiedShuffleSplit** with following parameters\n",
    "     - test_size = 0.08\n",
    "     - random_state = seed\n",
    "- Perform train test split with **StratifiedShuffleSplit** with following parameters\n",
    "     - test_size = 0.3\n",
    "     - random_state = seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=9\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "data_split = StratifiedShuffleSplit( labels_all,1, test_size=0.08,random_state=seed )\n",
    "for train_index, test_index in data_split.split(trainX, trainy):\n",
    "\n",
    "    split_data_92, split_data_8 = trainX[train_index], trainX[test_index]\n",
    "\n",
    "    split_label_92, split_label_8 = trainy[train_index], trainy[test_index]\n",
    "train_test_split = StratifiedShuffleSplit(  split_label_8,1, test_size=0.3,random_state=seed                      ) #test_size=0.3 denotes that 30 % of the dataset is used for testing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Splitting**\n",
    "- Print the shape of \n",
    "     - train_data\n",
    "     - train_labels\n",
    "     - test_data\n",
    "     - test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data :  (5400, 28, 28)\n",
      "train_labels :  (5400,)\n",
      "test_data :  (600, 28, 28)\n",
      "test_labels :  (600,)\n"
     ]
    }
   ],
   "source": [
    "for train_index, test_index in train_test_split.split(split_data_8,split_label_8):\n",
    "\n",
    "    train_data_70, test_data_30 = split_data_8[train_index], split_data_8[test_index]\n",
    "\n",
    "    train_label_70, test_label_30 = split_label_8[train_index], split_label_8[test_index]\n",
    "train_data = train_data_70 #assigning to variable train_data\n",
    "\n",
    "train_labels = train_label_70 #assigning to variable train_labels\n",
    "\n",
    "test_data = test_data_30\n",
    "\n",
    "test_labels = test_label_30\n",
    "print('train_data : ',     train_data.shape               )\n",
    "\n",
    "print('train_labels : ',      train_labels.shape           )\n",
    "\n",
    "print('test_data : ',          test_data.shape         )\n",
    "\n",
    "print('test_labels : ',      test_labels.shape          )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Normalization**\n",
    "- Apply the mean for **data** with following parameters\n",
    "     - axis = (0,1,2)\n",
    "     - keepdims=True\n",
    "- Apply the square root for **data** with following parameters\n",
    "     - axis = (0,1,2)\n",
    "     - ddof = 1\n",
    "     - keepdims = True\n",
    "- Print the shape of \n",
    "     - train_data\n",
    "     - test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data:  (5400, 28, 28)\n",
      "test_data:  (600, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "# definition of normalization function\n",
    "\n",
    "def normalize(data, eps=1e-8):\n",
    "\n",
    "    data -= data.mean(axis=(0, 1, 2), keepdims=True)\n",
    "\n",
    "    std = np.sqrt(data.var(axis=(0, 1, 2), ddof=1, keepdims=True))\n",
    "\n",
    "    std[std < eps] = 1.\n",
    "\n",
    "    data /= std\n",
    "\n",
    "    return data\n",
    "train_data=train_data.astype('float64')\n",
    "test_data=test_data.astype('float64')\n",
    "# calling the function\n",
    "\n",
    "train_data = normalize(train_data)\n",
    "\n",
    "test_data = normalize(test_data)\n",
    "# prints the shape of train data and test data\n",
    "\n",
    "print('train_data: ',      train_data.shape     )\n",
    "\n",
    "print('test_data: ',       test_data.shape    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ZCA Whitening**\n",
    "\n",
    "- Print the shape of \n",
    "     - train_data\n",
    "     - test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data_flat:  (784, 5400)\n",
      "test_data_flat:  (784, 600)\n"
     ]
    }
   ],
   "source": [
    "# Computing whitening matrix \n",
    "\n",
    "train_data_flat = train_data.reshape(train_data.shape[0], -1).T\n",
    "\n",
    "test_data_flat = test_data.reshape(test_data.shape[0], -1).T\n",
    "\n",
    "print('train_data_flat: ',    train_data_flat.shape      )\n",
    "\n",
    "print('test_data_flat: ',        test_data_flat.shape       )\n",
    "\n",
    "\n",
    "\n",
    "train_data_flat_t = train_data_flat.T\n",
    "\n",
    "test_data_flat_t = test_data_flat.T\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Principle Component Analysis (PCA)**\n",
    "- Keep **n_components** of **train_data_pca** as size of **train_data** columns and fit transform with **train_data_flat**\n",
    "- Keep **n_components** of **test_data_pca** as size of **test_data** columns and fit transform with **test_data_flat**\n",
    "- Print the shape of \n",
    "     - train_data_pca\n",
    "     - test_data_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 28)\n",
      "(784, 28)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# n_components specify the no.of components to keep\n",
    "\n",
    "train_data_pca = PCA(n_components=train_data.shape[1]).fit_transform(train_data_flat)\n",
    "test_data_pca = PCA(n_components=test_data.shape[1]).fit_transform(test_data_flat)\n",
    "\n",
    "print(    train_data_pca.shape             )\n",
    "\n",
    "print(     test_data_pca.shape         ) \n",
    "\n",
    "train_data_pca = train_data_pca.T\n",
    "\n",
    "test_data_pca = test_data_pca.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Singular Value Decomposition (SVD)**\n",
    "\n",
    "Execute the below cells to perform Singular Value Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: FutureWarning: The behavior of rgb2gray will change in scikit-image 0.19. Currently, rgb2gray allows 2D grayscale image to be passed as inputs and leaves them unmodified as outputs. Starting from version 0.19, 2D arrays will be treated as 1D images with 3 channels.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5400, 28)\n",
      "(600, 28)\n"
     ]
    }
   ],
   "source": [
    "from skimage import color\n",
    "def svdFeatures(input_data):\n",
    "\n",
    "    svdArray_input_data=[]\n",
    "\n",
    "    size = input_data.shape[0]\n",
    "\n",
    "    for i in range (0,size):\n",
    "\n",
    "        img=color.rgb2gray(input_data[i])\n",
    "\n",
    "        U, s, V = np.linalg.svd(img, full_matrices=False);\n",
    "\n",
    "        S=[s[i] for i in range(28)]\n",
    "\n",
    "        svdArray_input_data.append(S)\n",
    "\n",
    "        svdMatrix_input_data=np.matrix(svdArray_input_data)\n",
    "\n",
    "    return svdMatrix_input_data\n",
    "\n",
    "\n",
    "\n",
    "# apply SVD for train and test data\n",
    "\n",
    "train_data_svd=svdFeatures(train_data)\n",
    "\n",
    "test_data_svd=svdFeatures(test_data)\n",
    "print(train_data_svd.shape)\n",
    "print(test_data_svd.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Support Vector Machine (SVM)**\n",
    "- Initialize SVM classifier with following parameters\n",
    "    - gamma=.001\n",
    "    - probability=True\n",
    "\n",
    "- Train the model with train_data_flat_t and train_labels\n",
    "\n",
    "- Now predict the output with test_data_flat_t\n",
    "\n",
    "- Evaluate the classifier with score from test_data_flat_t and test_labels\n",
    "\n",
    "- Print the predicted score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm #Creating a svm classifier model\n",
    "\n",
    "clf = svm.SVC(    gamma=.001,probability=True                 ) #train_data_flat_tModel training\n",
    "\n",
    "train = clf.fit(train_data_flat_t, train_labels)\n",
    "predicted=\n",
    "\n",
    "score = \n",
    "print(\"score\",score)\n",
    "\n",
    "with open('output.txt', 'w') as file:\n",
    "    file.write(str(np.mean(score)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
